[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LAI Reading Group",
    "section": "",
<<<<<<< HEAD
    "text": "Lancaster AI (LAI) reading group is a weekly reading group focusing on topics related to AI, including but not limited to: diffusion models, information geometry, stochastic optimisation, geometric deep learning. It will be more tutorial-styled, instead of seminar-styled, tailored more towards people who wish to learn more about the recent developments of AI. This reading group is supported by the Prob_AI Hub.\nIn Term 2, we are doing reinforcement learning.\nIn Term 3, we will be doing geometric deep learning.\nFylde D31 and over Teams; Wednesday 2-3pm (mostly).\nSee here for the full schedules; here for past sessions.\n\nNext Session\n\n\n\n\n\n\n\n\n\n\nTitle\nLocation\nDate\nTime\nSpeaker\n\n\n\n\nMarkov Decision Processes and How to Solve Them\nPSC Lab 2\n29 Jan 2025\n2 pm -3 pm\nRui\n\n\n\nEmail Rui or Andreas for any question related to the reading group.\n\n\n\nSobbing GO Grandmaster Ke Jie (Globally No.1 at the time) after losing to AlphaGo"
=======
    "text": "Lancaster AI (LAI) reading group is a weekly reading group focusing on topics related to AI, including but not limited to: diffusion models, information geometry, stochastic optimisation, geometric deep learning. It will be more tutorial-styled, instead of seminar-styled, tailored more towards people who wish to learn more about the recent developments of AI. This reading group is supported by the Prob_AI Hub.\nIn Term 2, we are doing reinforcement learning.\nIn Term 3, we will be doing geometric deep learning.\nFylde D31 and over Teams; Wednesday 2-3pm (mostly).\nSee here for the full schedules; here for past sessions.\n\nNext Session\n\n\n\n\n\n\n\n\n\n\nTitle\nLocation\nDate\nTime\nSpeaker\n\n\n\n\nMarkov Decision Processes and How to Solve Them\nPSC Lab 2\n29 Jan 2024\n2pm - 3pm\nRui\n\n\n\nEmail Rui or Andreas for any question related to the reading group.\n\n\n\nSobbing GO Grandmaster Ke Jie (Globally No.1 at the time) after losing to AlphaGo"
>>>>>>> 011674306dcce0044044cae29085171a6cc1a89d
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The website is currently maintained by Rui Zhang and Andreas Makris.\nEmail Rui or Andreas for any question related to the reading group."
  },
  {
    "objectID": "about.html#current-members",
    "href": "about.html#current-members",
    "title": "About",
    "section": "Current Members",
    "text": "Current Members\n\nSoheil Arabzadeh\nEduard Campillo-Funollet\nLloyd Chapman\nTheo Crookes\nDan Dodd\nMiles Elvidge\nPaul Fearnhead\nYuga Iguchi\nDavid Leslie\nRobert Lambert\nAndreas Makris\nAlin Morariu\nHenry Moss\nThomas Newman\nChris Nemeth\nAdam Page\nTamás P. Papp\nChris Sherlock\nTheodore Tollet\nJack Trainer\nConnie Trojan\nLanya Yang\nRui Zhang\nKai Zheng"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Statistical exploration of the Manifold Hypothesis\n\n\n\n\n\nGeometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges"
  },
  {
    "objectID": "resources.html#diffusion-models-term-1-24-25",
    "href": "resources.html#diffusion-models-term-1-24-25",
    "title": "Resources",
    "section": "Diffusion Models (Term 1, 24-25)",
    "text": "Diffusion Models (Term 1, 24-25)\n\nTutorials\n\nStep-by-Step Diffusion: An Elementary Tutorial\nDenoising Diffusion Probabilistic Models in Six Simple Steps\nTutorial on Diffusion Models for Imaging and Vision\n\n\n\nPapers and Surveys\n\nScore-Based Generative Modeling through Stochastic Differential Equations\nDiffusion Models: A Comprehensive Survey of Methods and Applications\nHigh-Resolution Image Synthesis with Latent Diffusion Models\nA Unified Framework for U-Net Design and Analysis\n\n\n\nBlogs and Videos\n\nSong Yang’s Blog Post on Diffusion\nDiffusion Models | PyTorch Implementation\nDiffusion Models | Paper Explanation | Math Explained\nScore-based Generative Modeling in Latent Space"
  },
  {
    "objectID": "resources.html#general-machine-learning-textbooks",
    "href": "resources.html#general-machine-learning-textbooks",
    "title": "Resources",
    "section": "General Machine Learning Textbooks",
    "text": "General Machine Learning Textbooks\n\nProbabilistic Machine Learning 1 - Kevin Murphy\nProbabilistic Machine Learning 2 - Kevin Murphy\nMathematics for Machine Learning\nDeep Learning Foundations and Concepts - Chris and Hugh Bishop"
  },
  {
    "objectID": "about.html#schedule",
    "href": "about.html#schedule",
    "title": "About",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\n\n\nTitle\nLocation\nDate\nTime\nSpeaker\n\n\n\n\nCommercial Diffusion Models\nFylde D31 (and Teams)\n4 Dec 2024\n2pm - 3pm\nJack\n\n\nConditional Diffusions\nFylde D31 (and Teams)\n27 Nov 2024\n2pm - 3pm\nTamas\n\n\nLatent Diffusion\nFylde D31 (and Teams)\n20 Nov 2024\n2pm - 3pm\nLanya\n\n\nU Nets and NN Architectures\nFylde D31 (and Teams)\n13 Nov 2024\n2pm - 3pm\nJack\n\n\nScore Matching\nFylde D31 (and Teams)\n6 Nov 2024\n2pm - 3pm\nConnie\n\n\nDerivations of Diffusion Models\nFylde D31 (and Teams)\n30 Oct 2024\n2pm - 3pm\nAndreas\n\n\nIntroduction to Deep Learning\nFylde D31 (and Teams)\n23 Oct 2024\n2pm - 3pm\nAndreas\n\n\nA Brief Overview of Diffusion Models\nFylde D31 (and Teams)\n16 Oct 2024\n2.30pm - 3pm\nRui"
  },
  {
    "objectID": "index.html#next-session",
    "href": "index.html#next-session",
    "title": "LAI Reading Group",
    "section": "Next Session",
    "text": "Next Session\n\n\n\n\n\n\n\n\n\n\nTitle\nLocation\nDate\nTime\nSpeaker\n\n\n\n\nIntroduction to Deep Learning\nFylde D31 (and Teams)\n23 Oct 2024\n2pm - 3pm\nAndreas\n\n\n\n\n\n\nThe Most Famous Dog in Machine Learning"
  },
  {
    "objectID": "past.html",
    "href": "past.html",
    "title": "Past Sessions",
    "section": "",
    "text": "Title\nDate\nSpeaker\nReferences\n\n\n\n\n\nIntroduction to RL\n22 Jan 2024\nAndreas\nSlides"
  },
  {
    "objectID": "past.html#schedule",
    "href": "past.html#schedule",
    "title": "Past Sessions",
    "section": "",
    "text": "Title\nLocation\nDate\nTime\nSpeaker\n\n\n\n\nCommercial Diffusion Models\nFylde D31 (and Teams)\n4 Dec 2024\n2pm - 3pm\nJack\n\n\nConditional Diffusions\nFylde D31 (and Teams)\n27 Nov 2024\n2pm - 3pm\nTamas\n\n\nLatent Diffusion\nFylde D31 (and Teams)\n20 Nov 2024\n2pm - 3pm\nLanya\n\n\nU Nets and NN Architectures\nFylde D31 (and Teams)\n13 Nov 2024\n2pm - 3pm\nJack\n\n\nScore Matching\nFylde D31 (and Teams)\n6 Nov 2024\n2pm - 3pm\nConnie\n\n\nDerivations of Diffusion Models\nFylde D31 (and Teams)\n30 Oct 2024\n2pm - 3pm\nAndreas\n\n\nIntroduction to Deep Learning\nFylde D31 (and Teams)\n23 Oct 2024\n2pm - 3pm\nAndreas\n\n\nA Brief Overview of Diffusion Models\nFylde D31 (and Teams)\n16 Oct 2024\n2.30pm - 3pm\nRui"
  },
  {
    "objectID": "past.html#past-sessions",
    "href": "past.html#past-sessions",
    "title": "Past Sessions",
    "section": "",
    "text": "Title\nDate\nSpeaker\nReferences\n\n\n\n\n\nA Brief Overview of Diffusion Models\n16 Oct 2024\nRui\nScore-Based Generative Modeling through Stochastic Differential Equations  Diffusion Models: A Comprehensive Survey of Methods and Applications"
  },
  {
    "objectID": "schedules.html",
    "href": "schedules.html",
    "title": "Schedules",
    "section": "",
    "text": "Title\nLocation\nDate\nTime\nSpeaker\n\n\n\n\nCommercial Diffusion Models\nFylde D31\n4 Dec 2024\n2pm - 3pm\nJack\n\n\nLatent Diffusion\nFylde D31\n27 Nov 2024\n2pm - 3pm\nLanya\n\n\nConditional Diffusions\nFylde D31\n20 Nov 2024\n2pm - 3pm\nTamas\n\n\nU Nets and NN Architectures\nFylde D31\n13 Nov 2024\n2pm - 3pm\nJack\n\n\nScore Matching\nFylde D31\n6 Nov 2024\n2pm - 3pm\nConnie\n\n\nDenoising Diffusion Probabilistic Models\nFylde D31\n30 Oct 2024\n2pm - 3pm\nAndreas\n\n\nIntroduction to Deep Learning\nFylde D31\n23 Oct 2024\n2pm - 3pm\nAndreas\n\n\nA Brief Overview of Diffusion Models\nFylde D31\n16 Oct 2024\n2.30pm - 3pm\nRui"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Title\nLocation\nDate\nTime\nSpeaker\n\n\n\n\nImplementing Bayesian bandits in Python - a financial case study\nPSC Lab 2\n26 Feb 2025\n2 pm - 3 pm\nAlin\n\n\nAdvanced Bandit Algorithms\nFylde D31\n19 Feb 2025\n1 pm -2 pm\nJames Grant\n\n\nBandits and Exploration v.s. Exploitation\nPSC Lab 2\n12 Feb 2025\n2 pm -3 pm\nTheo C\n\n\nTabular RL and Q-Learning\nPSC Lab 2\n6 Feb 2025\n2 pm -3 pm\nAdam\n\n\nAmortized Bayesian Experimental Design: Principles and Implementation(s)\nPSC Lab 2\n5 Feb 2025\n2 pm -3 pm\nAdrien Corenflos\n\n\nMarkov Decision Processes and How to Solve Them\nPSC Lab 2\n29 Jan 2025\n2 pm -3 pm\nRui\n\n\nIntroduction to Reinforcement Learning\nFylde D31\n22 Jan 2025\n2 pm -3 pm\nAndreas"
  },
  {
    "objectID": "resources.html#geometric-deep-learning-term-3-24-25",
    "href": "resources.html#geometric-deep-learning-term-3-24-25",
    "title": "Resources",
    "section": "",
    "text": "Statistical exploration of the Manifold Hypothesis\n\n\n\n\n\nGeometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges"
  },
  {
    "objectID": "resources.html#reinforcement-learning-term-2-24-25",
    "href": "resources.html#reinforcement-learning-term-2-24-25",
    "title": "Resources",
    "section": "Reinforcement Learning (Term 2, 24-25)",
    "text": "Reinforcement Learning (Term 2, 24-25)\n\nPapers and Surveys\n\nAlphaGo\nAlphaFold\nReinforcement Learning: An Overview\n\n\n\nTextbooks\n\nReinforcement Learning: An Introduction\n\n\n\nLecture Series\n\nDeepMind x UCL | Deep Learning Lecture Series 2021\nCS285 Deep Reinforcement Learning"
  },
  {
    "objectID": "slides/Flow_mathching_demo.html",
    "href": "slides/Flow_mathching_demo.html",
    "title": "Conditional Flow Matching",
    "section": "",
    "text": "This notebook is a self-contained example of conditional flow matching. This notebook is taken from this Github repo https://github.com/atong01/conditional-flow-matching\nIn this notebook, we show how to map from a source distribution \\(q_0\\) to a target distribution \\(q_1\\): * Conditional Flow Matching (CFM) * This is equivalent to the basic (non-rectified) formulation of “Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow” (Liu et al. 2023) * Is similar to “Stochastic Interpolants” (Albergo et al. 2023) with a non-variance preserving interpolant. * Is similar to “Flow Matching” (Lipman et al. 2023) but conditions on both source and target. * Optimal Transport CFM (OT-CFM), which directly optimizes for dynamic optimal transport\nNote that this Flow Matching is different from the Generative Flow Network Flow Matching losses. Here we specifically regress against continuous flows, rather than matching inflows and outflows.\n\nimport math\nimport os\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport ot as pot\nimport torch\nimport torchdyn\nfrom torchdyn.core import NeuralODE\nfrom torchdyn.datasets import generate_moons\n\nsavedir = \"models/8gaussian-moons\"\nos.makedirs(savedir, exist_ok=True)\n\n\n# Implement some helper functions\n\n\ndef eight_normal_sample(n, dim, scale=1, var=1):\n    m = torch.distributions.multivariate_normal.MultivariateNormal(\n        torch.zeros(dim), math.sqrt(var) * torch.eye(dim)\n    )\n    centers = [\n        (1, 0),\n        (-1, 0),\n        (0, 1),\n        (0, -1),\n        (1.0 / np.sqrt(2), 1.0 / np.sqrt(2)),\n        (1.0 / np.sqrt(2), -1.0 / np.sqrt(2)),\n        (-1.0 / np.sqrt(2), 1.0 / np.sqrt(2)),\n        (-1.0 / np.sqrt(2), -1.0 / np.sqrt(2)),\n    ]\n    centers = torch.tensor(centers) * scale\n    noise = m.sample((n,))\n    multi = torch.multinomial(torch.ones(8), n, replacement=True)\n    data = []\n    for i in range(n):\n        data.append(centers[multi[i]] + noise[i])\n    data = torch.stack(data)\n    return data\n\n\ndef sample_moons(n):\n    x0, _ = generate_moons(n, noise=0.2)\n    return x0 * 3 - 1\n\n\ndef sample_8gaussians(n):\n    return eight_normal_sample(n, 2, scale=5, var=0.1).float()\n\n\nclass MLP(torch.nn.Module):\n    def __init__(self, dim, out_dim=None, w=64, time_varying=False):\n        super().__init__()\n        self.time_varying = time_varying\n        if out_dim is None:\n            out_dim = dim\n        self.net = torch.nn.Sequential(\n            torch.nn.Linear(dim + (1 if time_varying else 0), w),\n            torch.nn.SELU(),\n            torch.nn.Linear(w, w),\n            torch.nn.SELU(),\n            torch.nn.Linear(w, w),\n            torch.nn.SELU(),\n            torch.nn.Linear(w, out_dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass GradModel(torch.nn.Module):\n    def __init__(self, action):\n        super().__init__()\n        self.action = action\n\n    def forward(self, x):\n        x = x.requires_grad_(True)\n        grad = torch.autograd.grad(torch.sum(self.action(x)), x, create_graph=True)[0]\n        return grad[:, :-1]\n\n\nclass torch_wrapper(torch.nn.Module):\n    \"\"\"Wraps model to torchdyn compatible format.\"\"\"\n\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n\n    def forward(self, t, x, args=None):\n        return self.model(torch.cat([x, t.repeat(x.shape[0])[:, None]], 1))\n\n\ndef plot_trajectories(traj):\n    n = 2000\n    plt.figure(figsize=(6, 6))\n    plt.scatter(traj[0, :n, 0], traj[0, :n, 1], s=10, alpha=0.8, c=\"black\")\n    plt.scatter(traj[:, :n, 0], traj[:, :n, 1], s=0.2, alpha=0.2, c=\"olive\")\n    plt.scatter(traj[-1, :n, 0], traj[-1, :n, 1], s=4, alpha=1, c=\"blue\")\n    plt.legend([\"Prior sample z(S)\", \"Flow\", \"z(0)\"])\n    plt.xticks([])\n    plt.yticks([])\n    plt.show()\n\n\nConditional Flow Matching\nFirst we implement the basic conditional flow matching. As in the paper, we have \\[\n\\begin{align}\nz &= (x_0, x_1) \\\\\nq(z) &= q(x_0)q(x_1) \\\\\np_t(x | z) &= \\mathcal{N}(x | t * x_1 + (1 - t) * x_0, \\sigma^2) \\\\\nu_t(x | z) &= x_1 - x_0\n\\end{align}\n\\] When \\(\\sigma = 0\\) this is equivalent to zero-steps of rectified flow. We find that small \\(\\sigma\\) helps to regularize the problem ymmv.\n\n%%time\nsigma = 0.1\ndim = 2\nbatch_size = 256\nmodel = MLP(dim=dim, time_varying=True)\noptimizer = torch.optim.Adam(model.parameters())\n\nstart = time.time()\nfor k in range(20000):\n    optimizer.zero_grad()\n    t = torch.rand(batch_size, 1)\n    x0 = sample_8gaussians(batch_size)\n    x1 = sample_moons(batch_size)\n    mu_t = t * x1 + (1 - t) * x0\n    sigma_t = sigma\n    x = mu_t + sigma_t * torch.randn(batch_size, dim)\n    ut = x1 - x0\n    vt = model(torch.cat([x, t], dim=-1))\n    loss = torch.mean((vt - ut) ** 2)\n    loss.backward()\n    optimizer.step()\n    if (k + 1) % 5000 == 0:\n        end = time.time()\n        print(f\"{k+1}: loss {loss.item():0.3f} time {(end - start):0.2f}\")\n        start = end\n        node = NeuralODE(\n            torch_wrapper(model), solver=\"dopri5\", sensitivity=\"adjoint\", atol=1e-4, rtol=1e-4\n        )\n        with torch.no_grad():\n            traj = node.trajectory(\n                sample_8gaussians(1024),\n                t_span=torch.linspace(0, 1, 100),\n            )\n            plot_trajectories(traj)\ntorch.save(model, f\"{savedir}/cfm_v1.pt\")\n\n5000: loss 8.896 time 14.94\n\n\n\n\n\n\n\n\n\n10000: loss 8.825 time 15.87\n\n\n\n\n\n\n\n\n\n15000: loss 8.178 time 17.13\n\n\n\n\n\n\n\n\n\n20000: loss 8.456 time 19.18\n\n\n\n\n\n\n\n\n\nCPU times: user 3min 35s, sys: 26.3 s, total: 4min 1s\nWall time: 1min 7s\n\n\n\n\nOptimal Transport Conditional Flow Matching\nNext we implement optimal transport conditional flow matching. As in the paper, here we have \\[\n\\begin{align}\nz &= (x_0, x_1) \\\\\nq(z) &= \\pi(x_0, x_1) \\\\\np_t(x | z) &= \\mathcal{N}(x | t * x_1 + (1 - t) * x_0, \\sigma^2) \\\\\nu_t(x | z) &= x_1 - x_0\n\\end{align}\n\\] where \\(\\pi\\) is the joint of an exact optimal transport matrix. We first sample random \\(x_0, x_1\\), then resample according to the optimal transport matrix as computed with the python optimal transport package. We use the 2-Wasserstein distance with an \\(L^2\\) ground distance for equivalence with dynamic optimal transport.\n\n%%time\nsigma = 0.1\ndim = 2\nbatch_size = 256\nmodel = MLP(dim=dim, time_varying=True)\noptimizer = torch.optim.Adam(model.parameters())\n\nstart = time.time()\na, b = pot.unif(batch_size), pot.unif(batch_size)\nfor k in range(20000):\n    optimizer.zero_grad()\n    t = torch.rand(batch_size, 1)\n    x0 = sample_8gaussians(batch_size)\n    x1 = sample_moons(batch_size)\n\n    # Resample x0, x1 according to transport matrix\n    M = torch.cdist(x0, x1) ** 2\n    M = M / M.max()\n    pi = pot.emd(a, b, M.detach().cpu().numpy())\n    # Sample random interpolations on pi\n    p = pi.flatten()\n    p = p / p.sum()\n    choices = np.random.choice(pi.shape[0] * pi.shape[1], p=p, size=batch_size)\n    i, j = np.divmod(choices, pi.shape[1])\n    x0 = x0[i]\n    x1 = x1[j]\n    # calculate regression loss\n    mu_t = x0 * (1 - t) + x1 * t\n    sigma_t = sigma\n    x = mu_t + sigma_t * torch.randn(batch_size, dim)\n    ut = x1 - x0\n    vt = model(torch.cat([x, t], dim=-1))\n    loss = torch.mean((vt - ut) ** 2)\n    loss.backward()\n    optimizer.step()\n    if (k + 1) % 5000 == 0:\n        end = time.time()\n        print(f\"{k+1}: loss {loss.item():0.3f} time {(end - start):0.2f}\")\n        start = end\n        node = NeuralODE(\n            torch_wrapper(model), solver=\"dopri5\", sensitivity=\"adjoint\", atol=1e-4, rtol=1e-4\n        )\n        with torch.no_grad():\n            traj = node.trajectory(\n                sample_8gaussians(1024),\n                t_span=torch.linspace(0, 1, 100),\n            )\n            plot_trajectories(traj)\ntorch.save(model, f\"{savedir}/otcfm_v1.pt\")\n\n5000: loss 0.138 time 76.86\n\n\n\n\n\n\n\n\n\n10000: loss 0.103 time 75.88\n\n\n\n\n\n\n\n\n\n15000: loss 0.217 time 81.70\n\n\n\n\n\n\n\n\n\n20000: loss 0.114 time 86.51\n\n\n\n\n\n\n\n\n\nCPU times: user 17min 42s, sys: 1min 52s, total: 19min 34s\nWall time: 5min 21s"
  },
  {
    "objectID": "past.html#term-1-2425",
    "href": "past.html#term-1-2425",
    "title": "Past Sessions",
    "section": "Term 1, 24/25",
    "text": "Term 1, 24/25\n\n\n\nTitle\nDate\nSpeaker\nReferences\n\n\n\n\n\nCommercial Diffusion Models\n11 Dec 2024\nJack\nPaper\n\n\n\nFlow Matching\n27 Nov 2024\nChris N\nSlides  Code\n\n\n\nConditional Diffusions\n20 Nov 2024\nTamas\nSlides\n\n\n\nTransformers\n13 Nov 2024\nJack\nSlides  Paper; Video; Blog Technical Report Mathematical Intro\n\n\n\nScore Matching\n6 Nov 2024\nConnie\nSlides\n\n\n\nDDPMs\n30 Oct 2024\nAndreas\nSlides  Google Colab ; Paper Model Weights\n\n\n\nIntroduction to Deep Learning\n23 Oct 2024\nAndreas\nSlides  Google Colab ; Paper Introduction to PyTorch\n\n\n\nA Brief Overview of Diffusion Models\n16 Oct 2024\nRui\nScore-Based Diffusion paper  Diffusion Models Survey"
  },
  {
    "objectID": "past.html#term-2-2425",
    "href": "past.html#term-2-2425",
    "title": "Past Sessions",
    "section": "",
    "text": "Title\nDate\nSpeaker\nReferences\n\n\n\n\n\nIntroduction to RL\n22 Jan 2024\nAndreas\nSlides"
  }
]